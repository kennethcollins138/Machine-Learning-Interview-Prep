{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan Norm and Distance\n",
    "\n",
    "The Manhattan norm (also known as the L1 norm or taxicab norm) and Manhattan distance (also known as city block distance) are measures of distance in a vector space.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The Manhattan norm of a vector is the sum of the absolute values of its components. It gets its name from the grid layout of streets in Manhattan, which resembles a coordinate system. If you were to travel from one point to another in Manhattan, you would have to move along the grid lines (streets), so the distance you would travel (the Manhattan distance) is the sum of the horizontal and vertical distances.\n",
    "\n",
    "### Computation\n",
    "\n",
    "\n",
    "The Manhattan norm of a vector `x` in n-dimensional real or complex space is computed as:\n",
    "\n",
    "||x||1 = |x1| + |x2| + ... + |xn|\n",
    "\n",
    "\n",
    "The Manhattan distance between two points `x` and `y` is the Manhattan norm of the difference between the points:\n",
    "\n",
    "d(x, y) = ||x - y||1\n",
    "\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, the choice of norm can have a significant impact on your model. The Manhattan norm is less sensitive to outliers than the Euclidean norm (L2 norm), because it does not square the differences. This can be beneficial in situations where you want to minimize the influence of outliers.\n",
    "\n",
    "In neural networks, the L1 norm is often used in the context of regularization (L1 regularization) to encourage sparsity in the weights. This can help to prevent overfitting and improve interpretability.\n",
    "\n",
    "\n",
    "The Manhattan norm and distance are optimal to use when the data has a grid-like or lattice structure, when you want to minimize the influence of outliers, or when you want to encourage sparsity in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Norm and Distance\n",
    "\n",
    "The Euclidean norm (also known as the L2 norm or 2-norm) and Euclidean distance are measures of distance in a vector space.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The Euclidean norm of a vector is the length of the vector from the origin to the point represented by the vector. It's the straight-line distance, or \"as the crow flies\" distance. \n",
    "\n",
    "The Euclidean distance between two points is the length of the straight line between them. It's like the distance measured with a ruler between two points on a map.\n",
    "\n",
    "### Computation\n",
    "\n",
    "The Euclidean norm of a vector `x` in n-dimensional real or complex space is computed as:\n",
    "\n",
    "||x||2 = sqrt(x1^2 + x2^2 + ... + xn^2)\n",
    "\n",
    "\n",
    "The Euclidean distance between two points `x` and `y` is the Euclidean norm of the difference between the points:\n",
    "\n",
    "d(x, y) = ||x - y||2\n",
    "\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, the Euclidean norm and distance are often used in various algorithms, such as k-nearest neighbors (k-NN), where the Euclidean distance is used to find the k closest points to a given point.\n",
    "\n",
    "In neural networks, the L2 norm is often used in the context of regularization (L2 regularization or weight decay) to prevent overfitting by penalizing large weights.\n",
    "\n",
    "The Euclidean norm and distance are optimal to use when the data does not have a grid-like structure, when you want to penalize larger deviations more than smaller ones (due to the squaring of differences), or when you want to maintain the geometric properties of the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chebyshev Distance\n",
    "\n",
    "Chebyshev distance (also known as maximum value distance) is a metric defined on a vector space where the distance between two vectors is the greatest of their differences along any coordinate dimension.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine you're moving on a grid and you can move in 8 directions: horizontally, vertically, and diagonally. The Chebyshev distance between two points is the minimum number of moves you need to reach one point from the other. It's named after Pafnuty Chebyshev, a Russian mathematician.\n",
    "\n",
    "### Computation\n",
    "\n",
    "The Chebyshev distance between two points `x` and `y` in n-dimensional space is computed as:\n",
    "\n",
    "d(x, y) = max(|x1 - y1|, |x2 - y2|, ..., |xn - yn|)\n",
    "\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, Chebyshev distance can be used in clustering algorithms like k-means, or in nearest neighbors algorithms when the features have equal importance regardless of their range.\n",
    "\n",
    "The Chebyshev distance is optimal to use when the maximum difference along any dimension is of the greatest importance. It's also useful when you want to allow diagonal movements on a grid, as it gives the correct distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minkowski Distance\n",
    "\n",
    "Minkowski distance is a metric in a normed vector space which can be considered as a generalization of both Euclidean distance and Manhattan distance.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The Minkowski distance between two variables is a generalized metric form of Euclidean distance and Manhattan distance. It's named after the German mathematician Hermann Minkowski.\n",
    "\n",
    "### Computation\n",
    "\n",
    "The Minkowski distance of order `p` between two points `x` and `y` in n-dimensional real space is defined as:\n",
    "\n",
    "d(x, y) = (sum(|xi - yi|^p))^(1/p) for i = 1 to n\n",
    "\n",
    "\n",
    "When `p=1`, the Minkowski distance equals the Manhattan distance. When `p=2`, it equals the Euclidean distance.\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, Minkowski distance is used in various algorithms such as k-nearest neighbors (for both classification and regression), k-means and hierarchical clustering.\n",
    "\n",
    "The choice of the `p` parameter in the Minkowski distance can have a significant impact on these algorithms. A smaller `p` value puts less emphasis on large differences between feature values, while a larger `p` value puts more emphasis on large differences.\n",
    "\n",
    "The Minkowski distance is optimal to use when you need a flexible distance metric that can effectively become either the Manhattan or Euclidean distance based on the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming Distance\n",
    "\n",
    "Hamming distance is a metric used to measure the difference between two strings of equal length. It's named after Richard Hamming, an American mathematician and computer scientist.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The Hamming distance between two strings is the number of positions at which the corresponding symbols are different. It measures the minimum number of substitutions required to change one string into the other.\n",
    "\n",
    "### Computation\n",
    "\n",
    "The Hamming distance between two strings `s` and `t` of equal length is defined as:\n",
    "\n",
    "d(s, t) = sum(s[i] != t[i]) for i = 1 to n\n",
    "\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, Hamming distance is often used in algorithms that operate on binary data or categorical data. It can be used to measure the similarity between two binary vectors or two strings.\n",
    "\n",
    "In error detection and correction, Hamming distance is used to determine the number of bit flips required to correct an error.\n",
    "\n",
    "In AI, Hamming distance can be used in clustering algorithms to find similar patterns, or in genetic algorithms to measure the genetic difference between two solutions.\n",
    "\n",
    "Specific use cases include:\n",
    "\n",
    "- In information retrieval, Hamming distance can be used to find near-duplicate documents or to perform fuzzy string searching.\n",
    "- In bioinformatics, Hamming distance is used to align DNA sequences that have a similar length.\n",
    "- In computer vision, Hamming distance can be used to compare feature vectors that represent different images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It's often used to compare documents in text analysis.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The cosine similarity captures the angle between the two vectors. It's a judgement of orientation rather than magnitude. If the vectors are orthogonal (the angle between them is 90 degrees), they are less similar. If the vectors are in the same direction (the angle between them is 0 degrees), they are more similar.\n",
    "\n",
    "### Computation\n",
    "\n",
    "The cosine similarity between two vectors `a` and `b` is defined as:\n",
    "\n",
    "cosine_similarity(a, b) = dot_product(a, b) / (||a||2 * ||b||2)\n",
    "\n",
    "\n",
    "where `dot_product(a, b)` is the dot product of the vectors `a` and `b`, and `||a||2` and `||b||2` are the Euclidean lengths (L2 norm) of the vectors.\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, cosine similarity is a commonly used similarity measure for high-dimensional data. It's often used in text analysis to measure the similarity between documents or parts of documents.\n",
    "\n",
    "In neural networks, cosine similarity can be used as a measure of how similar the outputs of two different neurons are.\n",
    "\n",
    "In AI, cosine similarity can be used in recommendation systems to suggest products or services that are \"closest\" to the ones a user has already rated highly.\n",
    "\n",
    "Specific use cases include:\n",
    "\n",
    "- In information retrieval, cosine similarity can be used to rank documents by relevance to a search query.\n",
    "- In natural language processing, cosine similarity can be used to measure the semantic similarity of words or documents.\n",
    "- In computer vision, cosine similarity can be used to compare feature vectors that represent different images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Index\n",
    "\n",
    "The Jaccard Index, also known as the Jaccard similarity coefficient, is a statistic used for comparing the similarity and diversity of sample sets.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The Jaccard Index measures similarity between finite sample sets and is defined as the size of the intersection divided by the size of the union of the sample sets. It's a measure of how similar the two sets are.\n",
    "\n",
    "### Computation\n",
    "\n",
    "The Jaccard Index between two sets `A` and `B` is defined as:\n",
    "\n",
    "J(A, B) = |A ∩ B| / |A ∪ B|\n",
    "\n",
    "\n",
    "where `|A ∩ B|` is the size of the intersection of `A` and `B`, and `|A ∪ B|` is the size of the union of `A` and `B`.\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, the Jaccard Index is often used as a loss function for training classifiers. It's particularly popular in image segmentation tasks, where it's known as the Intersection over Union (IoU) score.\n",
    "\n",
    "In natural language processing, the Jaccard Index can be used to measure the similarity between documents or parts of documents.\n",
    "\n",
    "In AI, the Jaccard Index can be used in recommendation systems to find similar items or users.\n",
    "\n",
    "Specific use cases include:\n",
    "\n",
    "- In information retrieval, the Jaccard Index can be used to measure the similarity between documents.\n",
    "- In bioinformatics, the Jaccard Index can be used to compare the similarity and diversity of sample sets.\n",
    "- In computer vision, the Jaccard Index is used as a metric for evaluating the accuracy of image segmentation algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance\n",
    "\n",
    "The Mahalanobis distance is a measure of the distance between a point and a distribution. It's named after Prasanta Chandra Mahalanobis, an Indian scientist and statistician.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Unlike Euclidean distance, Mahalanobis distance takes into account the correlations of the data set and is scale-invariant. It measures distance relative to the centroid — a base or reference point that is the mean of all the input points.\n",
    "\n",
    "### Computation\n",
    "\n",
    "The Mahalanobis distance of a multivariate vector `x` from a group of values with mean `μ` and covariance matrix `S` is defined as:\n",
    "\n",
    "D(x) = sqrt((x - μ)T * S^-1 * (x - μ))\n",
    "\n",
    "\n",
    "where `T` denotes the transpose, and `S^-1` is the inverse covariance matrix.\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, Mahalanobis distance is often used in cluster analysis and classification techniques. It's particularly useful when working with multivariate data, when the variables have different units, or when the data contains outliers.\n",
    "\n",
    "In AI, Mahalanobis distance can be used in anomaly detection systems to identify outliers in multivariate data.\n",
    "\n",
    "Specific use cases include:\n",
    "\n",
    "- In multivariate anomaly detection, Mahalanobis distance is used to identify outliers in multivariate data where Euclidean distance can be misleading.\n",
    "- In classification problems, Mahalanobis distance can be used to compute the distance between a point and a distribution, rather than between individual points.\n",
    "- In computer vision, Mahalanobis distance can be used to recognize patterns or classify objects based on the covariance of the feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram-Schmidt Process\n",
    "\n",
    "The Gram-Schmidt process is a method for orthonormalizing a set of vectors in an inner product space, most commonly the Euclidean space R^n. It's named after Jørgen Pedersen Gram and Erhard Schmidt, two mathematicians who independently published this method.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The Gram-Schmidt process takes a finite, linearly independent set of vectors and generates an orthogonal or orthonormal (if the vectors are normalized) set of vectors that spans the same subspace as the original set.\n",
    "\n",
    "### Computation\n",
    "\n",
    "The Gram-Schmidt process is computed as follows:\n",
    "\n",
    "1. Start with a non-zero vector `v1`, normalize it to get the first vector `u1` in the orthonormal basis.\n",
    "2. For each subsequent vector `vi`, subtract the projection of `vi` onto all the previously computed vectors `u1, ..., ui-1`, and normalize the result to get `ui`.\n",
    "\n",
    "The mathematical formula for the Gram-Schmidt process is:\n",
    "\n",
    "ui = vi - sum((vi * uj) * uj for j = 1 to i-1)\n",
    "\n",
    "\n",
    "where `*` denotes the dot product.\n",
    "\n",
    "### Relation to Machine Learning, Neural Networks, and AI\n",
    "\n",
    "In machine learning, the Gram-Schmidt process is used in QR decomposition and the Gram-Schmidt orthogonalization process, which are used in various algorithms such as linear regression, least squares, and eigenvalue algorithms.\n",
    "\n",
    "In AI, the Gram-Schmidt process can be used in reinforcement learning to create an orthonormal basis for representing states or actions.\n",
    "\n",
    "Specific use cases include:\n",
    "\n",
    "- In linear algebra, the Gram-Schmidt process is used to find an orthonormal basis for a subspace of R^n.\n",
    "- In computer graphics, the Gram-Schmidt process is used to compute the camera coordinate system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

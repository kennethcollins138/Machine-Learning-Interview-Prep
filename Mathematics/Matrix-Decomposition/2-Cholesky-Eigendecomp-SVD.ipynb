{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky Decomposition\n",
    "Cholesky Decomposition is a method used to decompose a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose. It's essentially a square root of a matrix, specifically for positive-definite matrices.\n",
    "\n",
    "### Resources\n",
    "- [Explanation](https://www.geeksforgeeks.org/cholesky-decomposition-matrix-decomposition/)\n",
    "- [Example](https://www.youtube.com/watch?v=NppyUqgQqd0)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine you have a positive-definite matrix `A`. This matrix could represent something like the correlations between different variables in your dataset. Now, you want to \"simplify\" this matrix while keeping its essential features. This is where Cholesky Decomposition comes in.\n",
    "\n",
    "Cholesky Decomposition breaks down a positive-definite matrix into a product of a lower triangular matrix and its transpose. The lower triangular matrix is like a simpler \"building block\" that, when multiplied by its transpose, gives back the original matrix.\n",
    "\n",
    "Mathematically, if `A` is your positive-definite matrix, Cholesky Decomposition gives you a lower triangular matrix `L` such that `A = LL*`, where `L*` is the conjugate transpose of `L`.\n",
    "\n",
    "### Use Cases in Machine Learning\n",
    "\n",
    "1. **Covariance Matrix Decomposition**: Covariance matrices are positive-definite. Cholesky Decomposition allows us to break them down and work with simpler, lower-dimensional matrices, which can make computations more efficient and stable.\n",
    "\n",
    "2. **Multivariate Normal Distribution**: When generating samples from a multivariate normal distribution, we need to ensure they have the correct covariance structure. Cholesky Decomposition is used to achieve this.\n",
    "\n",
    "3. **Linear Least Squares for Multiple Outputs**: In multiple-output linear least squares problems, Cholesky Decomposition of the Gram matrix (a matrix representing inner products of vectors) can make the problem easier to solve.\n",
    "\n",
    "4. **Preconditioning**: In optimization, Cholesky Decomposition can be used to transform a problem into a form that's easier for algorithms like the conjugate gradient method to solve.\n",
    "\n",
    "5. **Kalman Filters**: Kalman filters are used in systems with linear dynamics to estimate the state of the system from noisy measurements. Cholesky Decomposition is used to update the covariance matrix, which is a key part of the Kalman filter algorithm.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Cholesky Decomposition is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose. It is named after André-Louis Cholesky, who found that real, symmetric, positive-definite matrices could be written as the product of a lower triangular matrix and its transpose.\n",
    "\n",
    "If `A` is a Hermitian, positive-definite matrix, then `A` can be written as `LL*` where `L` is a lower triangular matrix with real and positive diagonal entries, and `L*` is the conjugate transpose of `L`.\n",
    "\n",
    "### Use Cases in Machine Learning\n",
    "\n",
    "1. **Covariance Matrix Decomposition**: In statistics and machine learning, the covariance matrix of a dataset is often required to be decomposed for further analysis. Cholesky Decomposition is used here because the covariance matrix is a positive-definite matrix.\n",
    "\n",
    "2. **Multivariate Normal Distribution**: When generating samples from a multivariate normal distribution, the covariance matrix is decomposed using Cholesky Decomposition to transform independent standard normally distributed random variables to the desired covariance.\n",
    "\n",
    "3. **Linear Least Squares for Multiple Outputs**: When solving a linear least squares problem for multiple outputs, a Cholesky Decomposition of the Gram matrix is often used.\n",
    "\n",
    "4. **Preconditioning**: In optimization, Cholesky Decomposition is used as a preconditioner for the conjugate gradient method.\n",
    "\n",
    "5. **Kalman Filters**: In Kalman Filters, which are used for linear dynamical systems, Cholesky Decomposition is used to update the covariance matrix, which is a key part of the Kalman filter algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example, if you're working with a Gaussian (or normal) distribution, generating samples would mean creating data points that are spread out in a way that matches the properties of a Gaussian distribution - a certain mean (the peak of the distribution) and standard deviation (how wide the distribution is).\n",
    "\n",
    "This is a common task in machine learning and statistics, as it allows us to create synthetic data that behaves like the real-world data we're interested in. This can be useful for testing algorithms, making predictions, and understanding the behavior of different statistical distributions.\n",
    "\n",
    "In the context of the Cholesky decomposition example, generating samples from a multivariate Gaussian distribution means creating vectors of numbers where the distribution of vectors follows a multivariate Gaussian distribution. This distribution is specified by a mean vector (the average vector) and a covariance matrix (which indicates how each pair of elements in the vectors varies together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Desired mean and covariance\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.array([[1, 0.6], [0.6, 1]])\n",
    "\n",
    "# Cholesky decomposition of the covariance matrix\n",
    "L = np.linalg.cholesky(Sigma)\n",
    "\n",
    "# Generate samples\n",
    "n_samples = 1000\n",
    "Z = np.random.normal(size=(n_samples, 2))\n",
    "X = mu + np.dot(Z, L.T)\n",
    "\n",
    "# X is now a sample from a Gaussian distribution with mean mu and covariance Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigendecomposition and Diagonolization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resourcs\n",
    "- [Explain](https://builtin.com/data-science/eigendecomposition)\n",
    "- [Relation to PCA](https://pages.mtu.edu/~shanem/psy5220/daily/Day04/PCA.html#:~:text=Similarly%20V3%20is%20%27average%20of,the%20most%20to%20least%20important.)\n",
    "- [Great Code Demo](https://youtu.be/oshZQtYAh84?si=0O0nt2K-tqj2RW98)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Eigendecomposition is the process of breaking down a matrix into its constituent parts. It's a kind of 'factorization' where we represent the matrix in terms of its eigenvalues and eigenvectors. \n",
    "\n",
    "If `A` is a square matrix, we can write it as `A = PDP^-1`, where `D` is a diagonal matrix containing the eigenvalues of `A`, and `P` is a matrix whose columns are the eigenvectors of `A`. This is the eigendecomposition of `A`.\n",
    "\n",
    "Diagonalization is a special case of eigendecomposition. A matrix is diagonalizable if it is similar to a diagonal matrix, i.e., if we can find a basis of eigenvectors for the matrix. In other words, a matrix is diagonalizable if we can perform the eigendecomposition `A = PDP^-1` where `P` is invertible.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "The power of eigendecomposition and diagonalization lies in simplifying the matrix to make it easier to work with. Diagonal matrices have the property that they are easy to raise to a power or invert, which can be useful in many mathematical contexts.\n",
    "\n",
    "### Use Cases in Machine Learning\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**: PCA is a technique used to reduce the dimensionality of data. It works by computing the eigendecomposition of the data's covariance matrix, and using the eigenvectors (principal components) to project the data into a lower-dimensional space.\n",
    "\n",
    "2. **Spectral Clustering**: Spectral clustering techniques make use of the eigendecomposition of the data's similarity matrix to perform dimensionality reduction before clustering.\n",
    "\n",
    "3. **Linear Discriminant Analysis (LDA)**: LDA is a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. It involves computing the eigendecomposition of the within-class scatter matrix and the between-class scatter matrix.\n",
    "\n",
    "4. **Deep Learning**: In deep learning, eigendecomposition is used in the initialization of weights, optimization algorithms (like Adam and RMSProp), and understanding the model's capacity (like the spectral norm of a layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Eigendecomposition and the Determinant of a Matrix\n",
    "\n",
    "The determinant of a square matrix is a special number that can be calculated from its elements. It has two important properties related to eigendecomposition:\n",
    "\n",
    "1. **Determinant and Eigenvalues**: The determinant of a matrix is equal to the product of its eigenvalues (including their multiplicities). This is true regardless of the basis in which the matrix is expressed. If `A = PDP^-1` is the eigendecomposition of `A`, then `det(A) = det(D)`, where `det` denotes the determinant.\n",
    "\n",
    "2. **Determinant and Invariance**: The determinant of a matrix is invariant under change of basis. This means that if we perform an eigendecomposition or any other similarity transformation on the matrix, its determinant remains the same.\n",
    "\n",
    "These properties make the determinant a useful tool in linear algebra and matrix analysis, as it allows us to infer global properties of the matrix from its eigenvalues.\n",
    "## Relationship between Eigendecomposition and the Trace of a Matrix\n",
    "\n",
    "The trace of a square matrix is the sum of its diagonal elements. It has two important properties related to eigendecomposition:\n",
    "\n",
    "1. **Trace and Eigenvalues**: The trace of a matrix is equal to the sum of its eigenvalues (including their multiplicities). This is true regardless of the basis in which the matrix is expressed. If `A = PDP^-1` is the eigendecomposition of `A`, then `Tr(A) = Tr(D)`, where `Tr` denotes the trace.\n",
    "\n",
    "2. **Trace and Invariance**: The trace of a matrix is invariant under change of basis. This means that if we perform an eigendecomposition or any other similarity transformation on the matrix, its trace remains the same.\n",
    "\n",
    "These properties make the trace a useful tool in linear algebra and matrix analysis, as it allows us to infer global properties of the matrix from its eigenvalues.\n",
    "## Relationship between Eigendecomposition and Singular Value Decomposition (SVD)\n",
    "\n",
    "Eigendecomposition and SVD are both methods of factorizing a matrix, but they are used for different types of matrices and provide different insights.\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "Eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way.\n",
    "\n",
    "If `A` is a square `n x n` matrix, we can write it as `A = PDP^-1`, where `D` is a diagonal matrix containing the eigenvalues of `A`, and `P` is a matrix whose columns are the eigenvectors of `A`.\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD is a factorization of a real or complex matrix. It has many useful applications in signal processing and statistics.\n",
    "\n",
    "For a given `m x n` matrix `M`, the SVD is written as `M = UΣV*`, where `U` and `V` are unitary matrices and `Σ` is a diagonal matrix containing the singular values of `M`.\n",
    "\n",
    "### Relationship\n",
    "\n",
    "The main difference between the two is that SVD can be applied to any `m x n` matrix, whereas eigendecomposition can only be applied to diagonalizable matrices, which are often square.\n",
    "\n",
    "The columns of `U` in the SVD are actually the eigenvectors of `MM*`, and the columns of `V` are the eigenvectors of `M*M`. The singular values in `Σ` are the square roots of the eigenvalues from `M*M` or `MM*`.\n",
    "\n",
    "In other words, SVD is a generalization of the eigendecomposition to non-square matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "- [MUST WATCH](https://www.youtube.com/watch?v=_So8j8T-E1o&list=PLRDl2inPrWQW1QSWhBU0ki-jq_uElkh2a&index=42)\n",
    "- [Data Compression with SVD](https://www.youtube.com/watch?v=RscKCtF--NI&list=PLRDl2inPrWQW1QSWhBU0ki-jq_uElkh2a&index=43)\n",
    "### Intuition\n",
    "\n",
    "Singular Value Decomposition (SVD) is a method of decomposing a matrix into three other matrices. If `M` is an `m x n` matrix, the SVD is written as `M = UΣV*`, where:\n",
    "\n",
    "- `U` is an `m x m` unitary matrix\n",
    "- `Σ` is an `m x n` diagonal matrix\n",
    "- `V*` (the conjugate transpose of `V`) is an `n x n` unitary matrix\n",
    "\n",
    "The diagonal entries of `Σ` are the singular values of `M`. The columns of `U` are the left singular vectors (eigenvectors of `MM*`), and the columns of `V` are the right singular vectors (eigenvectors of `M*M`).\n",
    "\n",
    "### Explanation\n",
    "\n",
    "SVD provides a way to identify the orthogonal directions in the input space and the output space. The singular values (diagonal elements of `Σ`) tell us the amount of stretching or scaling that happens in these directions.\n",
    "\n",
    "### Use Cases in Machine Learning\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**: PCA can be performed using SVD. The right singular vectors `V` correspond to the principal components, and the singular values in `Σ` correspond to the square root of the eigenvalues of the covariance matrix.\n",
    "\n",
    "2. **Latent Semantic Analysis (LSA)**: In natural language processing, LSA uses SVD to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.\n",
    "\n",
    "3. **Image Compression**: SVD can be used for image compression by approximating an image matrix with a low-rank matrix.\n",
    "\n",
    "4. **Data Science**: SVD is used in data science to make predictions, fill in missing data, and find patterns in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Intuition of Singular Value Decomposition (SVD)\n",
    "\n",
    "The SVD of a matrix `M` is a factorization of the form `M = UΣV*`. Geometrically, this factorization represents a series of transformations to a given vector `x` in the input space:\n",
    "\n",
    "1. **Rotation or Reflection (`V*`)**: The first transformation is a rotation or reflection that aligns the axes of the input space with the principal axes of `M`. This is represented by the matrix `V*`, whose columns are the right singular vectors of `M`.\n",
    "\n",
    "2. **Scaling (`Σ`)**: The next transformation is a scaling that stretches or shrinks the vector along each of the principal axes. This is represented by the matrix `Σ`, which is a diagonal matrix containing the singular values of `M`. The singular values are the lengths of the semi-axes of the hyperellipse defined by `M`.\n",
    "\n",
    "3. **Rotation or Reflection (`U`)**: The final transformation is another rotation or reflection that maps the principal axes to the axes of the output space. This is represented by the matrix `U`, whose columns are the left singular vectors of `M`.\n",
    "\n",
    "In summary, any linear transformation represented by a matrix `M` can be decomposed into a rotation or reflection, a scaling, and another rotation or reflection. This is the geometric interpretation of the SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for whoever reads this:\n",
    "I had an easier time watching youtube videos explaining SVD which helped push my understanding. I reccomend going on a youtube haul and reading some papers to advance decomposition understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- 3Blue1Brown\n",
    "    - [Linear Combinations, span, and basis vectors](https://youtu.be/k7RM-ot2NWY)\n",
    "    - [Linear Transformations and matrices](https://youtu.be/kYB8IZa5AuE)\n",
    "    - [Matrix multiplication as composition](https://youtu.be/XkY2DOUCWMU)\n",
    "    - [The Determinant](https://youtu.be/Ip3X9LOh2dk)\n",
    "    - [Dot Products and duality](https://youtu.be/LyGKycYT2v0)\n",
    "    - [Cross Products](https://youtu.be/eu6i7WJeinw)\n",
    "    - [Change of Basis](https://youtu.be/P2LTAUO1TdA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector space is a mathematical structure formed by a collection of vectors. It is defined by the following eight axioms, which must hold for any vectors `u`, `v`, and `w` in the vector space, and any scalars `a` and `b`:\n",
    "\n",
    "1. **Associativity of addition:** `(u + v) + w = u + (v + w)`\n",
    "2. **Commutativity of addition:** `u + v = v + u`\n",
    "3. **Identity element of addition:** There exists an element `0` in the vector space such that `u + 0 = u` for every `u`.\n",
    "4. **Inverse elements of addition:** For every element `u`, there exists an element `-u` such that `u + (-u) = 0`.\n",
    "5. **Compatibility of scalar multiplication with field multiplication:** `a * (b * u) = (a * b) * u`\n",
    "6. **Identity element of scalar multiplication:** `1 * u = u`\n",
    "7. **Distributivity of scalar multiplication with respect to vector addition:** `a * (u + v) = a * u + a * v`\n",
    "8. **Distributivity of scalar multiplication with respect to scalar addition:** `(a + b) * u = a * u + b * u`\n",
    "\n",
    "These axioms provide the foundation for many concepts and operations in linear algebra, such as linear combinations, span, basis, and dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups in Vector Spaces\n",
    "\n",
    "In mathematics, a group is a set equipped with an operation that combines any two of its elements to form a third element in such a way that four conditions called group axioms are satisfied. These conditions are closure, associativity, identity and invertibility.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In the context of vector spaces, the set of all vectors forms a group under the operation of vector addition. This means that if you take any two vectors and add them together, you get another vector (closure). The operation of addition is associative, meaning that the order in which vectors are added does not matter. There is an identity element, the zero vector, which when added to any vector does not change the vector. Finally, every vector has an inverse (its negative), which when added to the vector gives the zero vector.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "A vector space is a set V together with two operations that satisfy the eight axioms (listed below). The operation \"+\" is called vector addition or simply addition. The operation \"â€¢\" is called scalar multiplication. \n",
    "\n",
    "1. Associativity of addition.\n",
    "2. Commutativity of addition.\n",
    "3. Identity element of addition.\n",
    "4. Inverse elements of addition.\n",
    "5. Compatibility of scalar multiplication with field multiplication.\n",
    "6. Identity element of scalar multiplication.\n",
    "7. Distributivity of scalar multiplication with respect to vector addition.\n",
    "8. Distributivity of scalar multiplication with respect to scalar addition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Mappings\n",
    "\n",
    "Linear mappings, also known as linear transformations or linear operators, are a fundamental concept in linear algebra.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The intuition behind linear mappings is that they are transformations that preserve the operations of vector addition and scalar multiplication. In other words, if you take any two vectors, their sum under the linear mapping is the same as if you mapped each vector individually and then took the sum. Similarly, the mapping of a scalar multiple of a vector is the same as if you mapped the vector and then took the scalar multiple.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Formally, a mapping `T` from a vector space `V` to a vector space `W` (both over the same field) is called a linear mapping if for every pair of vectors `u, v` in `V` and every scalar `c` in the field, the following two conditions hold:\n",
    "\n",
    "1. `T(u + v) = T(u) + T(v)` (preservation of addition)\n",
    "2. `T(cu) = cT(u)` (preservation of scalar multiplication)\n",
    "\n",
    "### Importance\n",
    "\n",
    "Linear mappings are important for several reasons:\n",
    "\n",
    "1. **Structure-Preserving:** By preserving the operations of vector addition and scalar multiplication, linear mappings preserve the structure of vector spaces. This makes them a key tool in the study of vector spaces and their properties.\n",
    "\n",
    "2. **Matrix Representation:** Every linear mapping can be represented by a matrix. This allows us to use the powerful tools of matrix algebra to study linear mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Valued Vector Spaces\n",
    "\n",
    "A real-valued vector space is a vector space where the field of scalars is the set of real numbers. This means that the vectors in the space can be multiplied by any real number, and the result is still a vector in the space. Examples of real-valued vector spaces include the Euclidean spaces R^n, where each vector is an ordered list of n real numbers.\n",
    "\n",
    "Real-valued vector spaces are fundamental in many areas of mathematics and its applications. For example, in calculus and differential equations, real-valued vector spaces are used to describe geometric and physical concepts such as velocity, force, and displacement. In linear algebra, real-valued vector spaces are used to study linear transformations, eigenvalues, and eigenvectors.\n",
    "\n",
    "## Abelian Groups\n",
    "\n",
    "An Abelian group, also known as a commutative group, is a set equipped with an operation (like addition or multiplication) that satisfies the following four conditions:\n",
    "\n",
    "1. **Closure:** If a and b are in the group, then a * b is also in the group.\n",
    "2. **Associativity:** (a * b) * c = a * (b * c) for all a, b, c in the group.\n",
    "3. **Identity:** There is an element e in the group such that a * e = e * a = a for all a in the group.\n",
    "4. **Invertibility:** For each element a in the group, there is an element b in the group such that a * b = b * a = e, where e is the identity element.\n",
    "\n",
    "The term \"Abelian\" comes from the name of the mathematician Niels Henrik Abel. The defining characteristic of an Abelian group is that its operation is commutative, that is, a * b = b * a for all a, b in the group.\n",
    "\n",
    "In the context of vector spaces, the set of all vectors forms an Abelian group under the operation of vector addition. This means that vector addition is commutative, which is a fundamental property used in many theorems and definitions in linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Product\n",
    "\n",
    "### Intuition\n",
    "The outer product of two vectors can be visualized as a matrix that represents all possible combinations of the elements of the two vectors. It's like creating a multiplication table for the two vectors.\n",
    "\n",
    "### Explanation\n",
    "The outer product of two vectors `u` and `v` is a matrix where each element is the product of an element from `u` and an element from `v`. If `u` is an `n`-dimensional column vector and `v` is an `m`-dimensional row vector, then the outer product of `u` and `v` is an `n x m` matrix.\n",
    "\n",
    "### Importance in Machine Learning, Neural Networks, and AI\n",
    "\n",
    "The outer product plays a crucial role in machine learning, neural networks, and AI:\n",
    "\n",
    "1. **Weight Update in Neural Networks:** In learning algorithms for neural networks, such as the Hebbian rule or the delta rule, the outer product is used to compute the update for the weight matrix.\n",
    "\n",
    "2. **Covariance Matrix:** The outer product is used in the computation of the covariance matrix, which is a key concept in statistics and machine learning. The covariance matrix is used in Principal Component Analysis (PCA), a technique for dimensionality reduction that is often used in machine learning.\n",
    "\n",
    "3. **Rank-1 Update:** The outer product results in a rank-1 matrix, which is used in rank-1 updates to matrices. This is a common operation in optimization algorithms used in machine learning, such as gradient descent.\n",
    "\n",
    "## Inner Product\n",
    "\n",
    "### Intuition\n",
    "The inner product of two vectors can be thought of as a measure of how much one vector \"goes in the direction of\" another vector. It's a way of combining two vectors to get a scalar.\n",
    "\n",
    "### Explanation\n",
    "The inner product (also known as the dot product) of two vectors `u` and `v` is a scalar obtained by multiplying corresponding entries of the two vectors and then summing those products. If `u` and `v` are both `n`-dimensional vectors, then their inner product is a scalar.\n",
    "\n",
    "## Dot Product\n",
    "\n",
    "### Intuition\n",
    "In the context of matrix multiplication, the dot product can be thought of as a way to combine rows and columns of matrices. It's a way of \"mixing\" the information in the matrices.\n",
    "\n",
    "### Explanation\n",
    "The dot product is the operation performed between rows of the first matrix and columns of the second matrix. If `A` is an `n x m` matrix and `B` is an `m x p` matrix, then the element in the `i`-th row and `j`-th column of the product `AB` is the dot product of the `i`-th row of `A` and the `j`-th column of `B`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose\n",
    "\n",
    "### Intuition\n",
    "The transpose of a matrix can be visualized as flipping the matrix over its diagonal. The rows of the original matrix become the columns of the transposed matrix, and the columns of the original matrix become the rows of the transposed matrix.\n",
    "\n",
    "### Explanation\n",
    "The transpose of a matrix `A` is denoted as `A^T`. If `A` is an `n x m` matrix, then `A^T` is an `m x n` matrix. The element in the `i`-th row and `j`-th column of `A^T` is the element in the `j`-th row and `i`-th column of `A`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transpose operation in linear algebra has several key properties:\n",
    "\n",
    "1. **Double Transpose:** The transpose of the transpose of a matrix is the original matrix. If `A` is a matrix, then `(A^T)^T = A`.\n",
    "\n",
    "2. **Transpose of a Sum:** The transpose of the sum of two matrices is the sum of their transposes. If `A` and `B` are matrices of the same size, then `(A + B)^T = A^T + B^T`.\n",
    "\n",
    "3. **Transpose of a Scalar Multiple:** The transpose of a scalar multiple of a matrix is the scalar multiple of the transpose of the matrix. If `A` is a matrix and `c` is a scalar, then `(cA)^T = c(A^T)`.\n",
    "\n",
    "4. **Transpose of a Product:** The transpose of the product of two matrices is the product of their transposes in reverse order. If `A` and `B` are matrices such that the product `AB` is defined, then `(AB)^T = B^T A^T`.\n",
    "\n",
    "5. **Symmetric Matrices:** A matrix is symmetric if it is equal to its transpose. If `A` is a matrix, then `A` is symmetric if and only if `A = A^T`.\n",
    "\n",
    "These properties are fundamental in many areas of linear algebra and its applications, including machine learning, neural networks, and AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Subspaces\n",
    "\n",
    "### Intuition\n",
    "A vector subspace can be thought of as a \"space within a space\". It's a subset of a vector space that still behaves like a vector space. For example, in a 3-dimensional space, a plane or a line passing through the origin is a subspace.\n",
    "\n",
    "### Explanation\n",
    "A vector subspace is a subset of a vector space that is closed under vector addition and scalar multiplication. This means that if you take any two vectors in the subspace and add them together, the result is still in the subspace. Similarly, if you take any vector in the subspace and multiply it by a scalar, the result is still in the subspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Subspacse\n",
    "A vector subspace is a subset of a vector space that satisfies three key properties:\n",
    "\n",
    "1. **Zero Vector:** The zero vector of the larger vector space is in the subspace. This is because the zero vector is needed for the subspace to have an additive identity.\n",
    "\n",
    "2. **Closed under Addition:** If you take any two vectors in the subspace and add them together, the result is still in the subspace. This property ensures that the subspace is self-contained under the operation of vector addition.\n",
    "\n",
    "3. **Closed under Scalar Multiplication:** If you take any vector in the subspace and multiply it by a scalar, the result is still in the subspace. This property ensures that the subspace is self-contained under the operation of scalar multiplication.\n",
    "\n",
    "These properties ensure that the subspace itself forms a vector space, with the same rules of vector addition and scalar multiplication as the larger vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Space of Homogenous System\n",
    "A homogeneous system of linear equations is a system of linear equations where all of the constant terms are zero. The general form of such a system is `Ax = 0`, where `A` is a matrix and `x` is a vector of variables.\n",
    "\n",
    "The solution space of a homogeneous system of linear equations is the set of all vectors `x` that satisfy the equation `Ax = 0`. This set of solutions forms a vector subspace, because it satisfies the three properties of a vector subspace:\n",
    "\n",
    "1. **Zero Vector:** The zero vector is always a solution to the homogeneous system `Ax = 0`, because `A*0 = 0`.\n",
    "\n",
    "2. **Closed under Addition:** If `x` and `y` are solutions to `Ax = 0`, then `x + y` is also a solution, because `A(x + y) = Ax + Ay = 0 + 0 = 0`.\n",
    "\n",
    "3. **Closed under Scalar Multiplication:** If `x` is a solution to `Ax = 0` and `c` is any scalar, then `cx` is also a solution, because `A(cx) = c(Ax) = c*0 = 0`.\n",
    "\n",
    "So, when we say that every subspace is the solution space of a homogeneous system of linear equations, we mean that for every subspace, there exists a homogeneous system of linear equations whose solution space is exactly that subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Independence\n",
    "\n",
    "### Intuition\n",
    "The concept of linear independence is about the uniqueness of representation. A set of vectors is linearly independent if no vector in the set can be represented as a linear combination of the other vectors. This means that each vector in the set adds a new \"dimension\" or direction that cannot be reached by any combination of the other vectors.\n",
    "\n",
    "### Explanation\n",
    "A set of vectors `{v1, v2, ..., vn}` is linearly independent if the only solution to the equation `c1*v1 + c2*v2 + ... + cn*vn = 0` is `c1 = c2 = ... = cn = 0`. In other words, the zero vector can only be represented as a combination of the vectors in the set by using all zero coefficients.\n",
    "\n",
    "### Example\n",
    "Consider the vectors `v1 = [1, 0]` and `v2 = [0, 1]` in 2-dimensional space. These vectors are linearly independent because there's no way to represent `v1` as a multiple of `v2`, or vice versa. The only way to represent the zero vector as a combination of `v1` and `v2` is by using zero coefficients: `0*v1 + 0*v2 = 0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Linear Independence\n",
    "\n",
    "1. **Nontrivial Linear Combination:** If a set of vectors is linearly independent, then the only solution to the equation `c1*v1 + c2*v2 + ... + cn*vn = 0` is `c1 = c2 = ... = cn = 0`. This means that the zero vector can only be represented as a trivial linear combination (i.e., with all coefficients equal to zero) of the vectors in the set.\n",
    "\n",
    "2. **Adding a Vector:** If a set of vectors is linearly independent, and a new vector is added to the set that is not a linear combination of the existing vectors, then the enlarged set is also linearly independent.\n",
    "\n",
    "3. **Removing a Vector:** If a set of vectors is linearly independent, and a vector is removed from the set, then the reduced set is still linearly independent.\n",
    "\n",
    "4. **Linear Independence of Subsets:** If a set of vectors is linearly independent, then any subset of that set is also linearly independent.\n",
    "\n",
    "5. **Size and Dimension:** In an n-dimensional vector space, any set of more than n vectors is linearly dependent, and any set of n linearly independent vectors forms a basis for the space.\n",
    "\n",
    "These properties are fundamental in many areas of linear algebra and its applications, including machine learning, neural networks, and AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between Linear Independence and Dimensionality\n",
    "\n",
    "The **dimension** of a vector space is defined as the maximum number of linearly independent vectors in the space. In other words, it's the size of the largest set of vectors that can all \"point in different directions\" (i.e., no vector in the set can be written as a linear combination of the others).\n",
    "\n",
    "A set of vectors is **linearly dependent** if at least one vector in the set can be written as a linear combination of the others. This means that the set has \"redundant\" vectors that don't increase the dimension of the space spanned by the set.\n",
    "\n",
    "So, the relationship between linear dependence and the dimension of a vector space is that a set of vectors is linearly dependent if and only if its size is greater than the dimension of the space it spans. Conversely, a set of vectors is linearly independent if and only if its size is less than or equal to the dimension of the space it spans.\n",
    "\n",
    "This relationship is fundamental in many areas of linear algebra and its applications, including machine learning, neural networks, and AI, where it's often important to identify a basis (a linearly independent set that spans the space) for a vector space, which will have a size equal to the dimension of the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis and Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Set and Span\n",
    "\n",
    "### Intuition\n",
    "The concept of a generating set and span is about the reachability of a vector space. A generating set for a vector space is a set of vectors such that every vector in the space can be written as a linear combination of vectors from the set. The span of a set of vectors is the set of all vectors that can be written as a linear combination of the vectors in the set.\n",
    "\n",
    "### Explanation\n",
    "A set of vectors `{v1, v2, ..., vn}` is a generating set for a vector space if every vector in the space can be written as `c1*v1 + c2*v2 + ... + cn*vn` for some scalars `c1, c2, ..., cn`. The span of a set of vectors is the set of all vectors that can be written in this way.\n",
    "\n",
    "### Example\n",
    "Consider the vectors `v1 = [1, 0]` and `v2 = [0, 1]` in 2-dimensional space. These vectors form a generating set for the space, because any vector `[x, y]` can be written as `x*v1 + y*v2`. The span of `v1` and `v2` is the entire 2-dimensional space.\n",
    "\n",
    "### Properties\n",
    "1. **Subspace:** The span of any set of vectors is a subspace of the vector space.\n",
    "2. **Smallest Subspace:** The span of a set of vectors is the smallest subspace that contains the set.\n",
    "3. **Generating Set:** A vector space is equal to the span of any of its generating sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between span and linear independence\n",
    "\n",
    "**Span** refers to the set of all vectors that can be created by taking linear combinations of a given set of vectors. In other words, the span of a set of vectors is the entire vector space that those vectors can reach through scaling and addition.\n",
    "\n",
    "**Linear Independence**, on the other hand, refers to a condition where no vector in a set can be written as a linear combination of the other vectors in the set. This means that each vector in a linearly independent set points in a \"new direction\" that can't be reached by any combination of the other vectors.\n",
    "\n",
    "The relationship between these two concepts is that a set of vectors spans a space if it reaches every point in the space, and it is linearly independent if it does so in a \"minimal\" way, without any redundancy. \n",
    "\n",
    "In other words, a set of vectors forms a basis for a vector space if it is both linearly independent (no redundancy) and spans the space (reaches every point). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis\n",
    "\n",
    "### Intuition\n",
    "A basis of a vector space is a set of vectors that, in a sense, define the space. A basis is a set of linearly independent vectors that span the entire vector space. This means that every vector in the space can be expressed as a unique linear combination of the basis vectors.\n",
    "\n",
    "### Explanation\n",
    "A set of vectors `{v1, v2, ..., vn}` forms a basis for a vector space if they are linearly independent and their span is the entire space. This means that every vector in the space can be written uniquely as `c1*v1 + c2*v2 + ... + cn*vn` for some scalars `c1, c2, ..., cn`.\n",
    "\n",
    "### Example\n",
    "Consider the vectors `v1 = [1, 0]` and `v2 = [0, 1]` in 2-dimensional space. These vectors form a basis for the space, because they are linearly independent and any vector `[x, y]` can be written as `x*v1 + y*v2`.\n",
    "\n",
    "### Properties\n",
    "1. **Uniqueness of Representation:** Every vector in the space can be represented in a unique way as a linear combination of the basis vectors.\n",
    "2. **Size and Dimension:** The number of vectors in a basis for a space is equal to the dimension of the space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between basis and Dimensionality\n",
    "\n",
    "The relationship between a basis and the dimension of a vector space is direct and straightforward: the dimension of a vector space is defined as the number of vectors in any basis for the space.\n",
    "\n",
    "A **basis** of a vector space is a set of vectors that are linearly independent and span the space. This means that every vector in the space can be expressed as a unique linear combination of the basis vectors.\n",
    "\n",
    "The **dimension** of a vector space is a measure of the \"size\" of the space. More formally, it's defined as the maximum number of linearly independent vectors in the space. \n",
    "\n",
    "Since a basis is a set of linearly independent vectors that span the space, the number of vectors in a basis gives you the dimension of the space. For example, in a 3-dimensional space, any basis for the space will consist of 3 vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank\n",
    "\n",
    "### Definition\n",
    "The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. \n",
    "\n",
    "### Intuition\n",
    "The rank gives a measure of the \"dimensionality\" of the data represented by the matrix. It tells us how many independent pieces of information are in the matrix, or equivalently, the dimension of the vector space spanned by the rows or columns of the matrix.\n",
    "\n",
    "### Explanation\n",
    "The rank of a matrix is found by reducing the matrix to its row echelon form (or even reduced row echelon form) and counting the number of non-zero rows. \n",
    "\n",
    "### Example\n",
    "Consider the matrix A = [[1, 2, 0], [2, 4, 0], [3, 6, 1]]. The first two rows are linearly dependent (the second row is twice the first), but the third row is linearly independent from the others. So, the rank of A is 2.\n",
    "\n",
    "### Properties\n",
    "1. **Rank and Invertibility:** A square matrix is invertible if and only if its rank is equal to its size (i.e., it has full rank).\n",
    "2. **Rank and Solutions to Systems of Equations:** The rank of a matrix determines the number of solutions to a system of linear equations represented by the matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship of Rank and Dimensionality\n",
    "The rank of a matrix is exactly equal to the dimension of its column space. \n",
    "\n",
    "The **rank** of a matrix is defined as the maximum number of linearly independent columns (or equivalently, rows) in the matrix. \n",
    "\n",
    "The **column space** (also called the range or image) of a matrix is the set of all possible linear combinations of its column vectors. \n",
    "\n",
    "So, the dimension of the column space is the number of linearly independent columns, which is by definition the rank of the matrix."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

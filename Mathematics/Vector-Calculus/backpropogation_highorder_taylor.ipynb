{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in order to create accurate models, we have to be able to continuously improve the model by finding parameters that optimize it.\n",
    "- for a objective function, we can obtain the gradient with respect to parameters using calculus and applying the chain rule\n",
    "- the computation of the gradient can be very computational expensive and slow which is its important to use efficient algorithms to compute the gradient\n",
    "- this is where backpropogation comes in\n",
    "- backpropagation is a method to compute the gradient of the loss function with respect to the parameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients in a Deep Network\n",
    "\n",
    "1. **Forward Propagation:** This is like the network making its best guess. It takes your input and runs it through all the layers, each of which is like a filter that progressively extracts higher-level features. For example, in a network trained to recognize images, the first layer might recognize edges, the next layer might recognize shapes made up of these edges, and so on.\n",
    "\n",
    "2. **Compute Loss:** This is the network's self-evaluation. It compares its own guess (the output from forward propagation) to the correct answer, and the difference is the \"loss\". The goal of the network is to be a better guesser, which means reducing this loss.\n",
    "\n",
    "3. **Backward Propagation (Backpropagation):** This is where the network learns from its mistakes. It looks at how far off its guess was (the loss) and traces back through its layers to see which weights contributed most to the error. It's like the network asking, \"Where did I go wrong?\" and identifying the culprits.\n",
    "\n",
    "4. **Update Weights:** Now that the network knows which weights led it astray, it adjusts them slightly so they'll contribute less to the error next time. This is like the network learning from its mistakes and resolving to do better in the future.\n",
    "\n",
    "The beauty of this process is that it's all done automatically. The network essentially teaches itself to make better predictions by learning from its errors. And the more data it sees, the better it gets at this. This is the essence of machine learning.\n",
    "\n",
    "- In order to grab the gradients with respect to a parameter, we need to use partial derivatives with respect to each individual parameter of each layer in the network. This is where the chain rule comes in. The chain rule is a way to compute the derivative of a function inside another function. We end up having the term furthest to the right being the derivative of the outer function with respect to the inner function, and the term furthest to the left being the derivative of the inner function with respect to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Automatic differentiation (also known as autodiff) is a set of techniques to numerically evaluate the derivative of a function. Here's the intuition:\n",
    "\n",
    "1. **Function Evaluation:** Autodiff works by breaking down complex equations into simple, elementary operations (like addition, multiplication, or elementary functions like exp, log, sin, etc.). This is similar to how a calculator performs computations.\n",
    "\n",
    "2. **Chain Rule:** The magic of autodiff comes from the chain rule, a basic principle from calculus. The chain rule allows us to compute the derivative of a composite function. Autodiff applies the chain rule repeatedly to these elementary operations, hence efficiently computing derivatives.\n",
    "\n",
    "3. **Forward and Reverse Modes:** Autodiff can be done in two modes: forward and reverse. Forward mode is efficient when we have many inputs and few outputs (like in a function that takes a vector and returns a scalar), while reverse mode is efficient when we have few inputs and many outputs (like in a function that takes a scalar and returns a vector). Reverse mode autodiff is what's commonly used in deep learning.\n",
    "\n",
    "4. **Computational Graphs:** In practice, autodiff is often implemented using computational graphs. A computational graph is a directed graph where nodes correspond to variables or operations. This allows us to keep track of the computations and apply the chain rule efficiently.\n",
    "\n",
    "The power of autodiff is that it allows us to compute derivatives exactly to machine precision, which is crucial in machine learning and especially in neural networks, where we need to compute gradients to update the model's parameters.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Function Evaluation:** Automatic differentiation starts by breaking down a complex function into a sequence of simple, elementary operations that can be easily differentiated, such as addition, multiplication, or elementary functions like exponential, logarithm, sine, etc. This is similar to how a calculator performs computations, step by step.\n",
    "\n",
    "2. **Chain Rule:** The core of automatic differentiation is the application of the chain rule, a fundamental principle from calculus. The chain rule allows us to compute the derivative of a composite function. In the context of automatic differentiation, it's applied repeatedly to these elementary operations, allowing us to efficiently compute derivatives of complex functions.\n",
    "\n",
    "3. **Forward and Reverse Modes:** Automatic differentiation can be performed in two modes: forward and reverse. In forward mode, we compute the derivative of each operation with respect to the inputs, moving from the input layer to the output layer. This is efficient when we have many inputs and few outputs. In reverse mode, we compute the derivative of each operation with respect to the outputs, moving from the output layer to the input layer. This is efficient when we have few inputs and many outputs. Reverse mode automatic differentiation, also known as backpropagation, is the method commonly used in deep learning.\n",
    "\n",
    "4. **Computational Graphs:** In practice, automatic differentiation is often implemented using computational graphs. A computational graph is a directed graph where nodes correspond to variables or operations. By representing our function as a computational graph, we can keep track of the computations and apply the chain rule efficiently. Each node in the graph calculates the derivative of its output with respect to its inputs, and passes this information to the next node.\n",
    "\n",
    "The power of automatic differentiation is that it allows us to compute derivatives exactly to machine precision, which is crucial in machine learning and especially in neural networks, where we need to compute gradients to update the model's parameters. This is what enables us to train complex models with millions or even billions of parameters.\n",
    "\n",
    "### Forward Mode\n",
    "\n",
    "In forward mode automatic differentiation, we compute the derivative of each operation with respect to the inputs, moving from the input layer to the output layer. This is efficient when we have many inputs and few outputs. \n",
    "\n",
    "Imagine you have a function `f(x, y, z) = u(x, y) * v(y, z)`. In forward mode, we would first compute the derivatives of `u` with respect to `x` and `y`, and `v` with respect to `y` and `z`. Then, using the chain rule, we would compute the derivative of `f` with respect to `x`, `y`, and `z`.\n",
    "\n",
    "### Reverse Mode\n",
    "\n",
    "In reverse mode automatic differentiation, we compute the derivative of each operation with respect to the outputs, moving from the output layer to the input layer. This is efficient when we have few inputs and many outputs. Reverse mode automatic differentiation, also known as backpropagation, is the method commonly used in deep learning.\n",
    "\n",
    "Continuing with the function `f(x, y, z) = u(x, y) * v(y, z)`, in reverse mode, we would first compute the derivative of `f` with respect to `u` and `v`. Then, using the chain rule, we would compute the derivative of `u` with respect to `x` and `y`, and `v` with respect to `y` and `z`.\n",
    "\n",
    "The key difference between forward mode and reverse mode is the order in which the derivatives are computed and the efficiency for different types of functions. Forward mode is more efficient for functions with many inputs and few outputs, while reverse mode is more efficient for functions with few inputs and many outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher-Order Derivatives\n",
    "\n",
    "Automatic differentiation can be extended to compute higher-order derivatives, such as second or third derivatives. This is useful in optimization algorithms that require the Hessian matrix, which contains the second derivatives of a function. Higher-order derivatives can also be useful in some machine learning models, such as in natural language processing or physics simulations.\n",
    "\n",
    "The Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables. The Hessian matrix is named after the German mathematician Ludwig Otto Hessian, who first introduced it in 1873.\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "Imagine you're hiking on a hill. The gradient of the hill gives you the steepest ascent direction - if you follow it, you'll climb the hill as quickly as possible. The Hessian, on the other hand, tells you how the hill is curved - whether it's a steep mountain, a gentle slope, or a saddle point.\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "The Hessian matrix of a function `f(x, y, ..., z)` is a matrix `H` where each element `H[i][j]` is the second partial derivative of `f` with respect to the `i`-th and `j`-th variable. In other words, it measures how the partial derivative of the function changes as we change our inputs.\n",
    "\n",
    "The Hessian matrix is used in optimization to determine if a critical point is a local maximum, local minimum, or a saddle point. If the Hessian is positive definite at a critical point, the function attains a local minimum at that point. If the Hessian is negative definite, the function attains a local maximum. If the Hessian has both positive and negative eigenvalues, the point is a saddle point.\n",
    "\n",
    "In machine learning, the Hessian is often used in second-order optimization methods, like Newton's method, which can converge faster than first-order methods like gradient descent, especially for functions that are poorly scaled or ill-conditioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearization and Multivariate Taylor Series\n",
    "\n",
    "Linearization is the process of approximating a function near a point by a linear function - the tangent line. This is based on the idea that a smooth, differentiable function appears almost linear when zoomed in sufficiently close to a point.\n",
    "\n",
    "The Taylor series is a representation of a function as an infinite sum of terms calculated from the values of its derivatives at a single point. The multivariate Taylor series is a generalization of this concept for functions with multiple variables.\n",
    "\n",
    "### Intuition and Relevance to Gradients/Backpropagation\n",
    "\n",
    "Imagine you're standing on a smooth hill and you look at the ground right in front of your feet. Even if the hill is very complex, the small patch of ground you're looking at appears almost flat - that's the idea of linearization.\n",
    "\n",
    "In the context of machine learning and backpropagation, we often use the gradient (the multi-dimensional analogue of the derivative) to perform linearization. The gradient points in the direction of steepest ascent and its magnitude gives the rate of increase in that direction. This linear approximation is crucial for gradient-based optimization methods like gradient descent.\n",
    "\n",
    "The multivariate Taylor series is like a polynomial approximation of a function, but in multiple dimensions. The first-order term in the Taylor series gives a linear approximation (the tangent plane), the second-order term takes into account curvature (similar to the role of the second derivative in the univariate case), and so on.\n",
    "\n",
    "The multivariate Taylor series is particularly relevant for understanding why gradient descent works. The first-order Taylor approximation of the loss function is used to decide the direction to move the parameters in order to decrease the loss. This is essentially what happens in each step of gradient descent: we compute the gradient (the first-order term in the Taylor series), and update the parameters in the opposite direction.\n",
    "\n",
    "Moreover, higher-order terms in the Taylor series are related to the concept of curvature, which is important in second-order optimization methods. For example, the Hessian (the matrix of second-order partial derivatives) appears in the second-order term of the Taylor series, and is used in methods like Newton's method to take into account the curvature of the loss surface."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

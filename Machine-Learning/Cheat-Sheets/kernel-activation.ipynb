{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation/Kernel Functions Cheat Sheet\n",
    "\n",
    "### Radial Basis Function (RBF)\n",
    "- Use when: You need a non-linear transformation in SVMs or need a universal function approximator in neural networks.\n",
    "- Pros: Can map data to infinite dimensions, good for handling non-linear classification.\n",
    "- Cons: Requires selection of a suitable parameter for spread.\n",
    "\n",
    "### Linear Function\n",
    "- Use when: Your data is linearly separable (for SVMs) or you need a simple transformation of the input (for neural networks).\n",
    "- Pros: Simple and computationally efficient.\n",
    "- Cons: Cannot handle non-linear data or complex tasks.\n",
    "\n",
    "### Polynomial Function\n",
    "- Use when: You need a non-linear transformation that considers interactions between features (for SVMs) or you need a non-linear activation function (for neural networks).\n",
    "- Pros: Can handle non-linear data, considers interactions between features.\n",
    "- Cons: Computationally expensive, may lead to overfitting with high degrees.\n",
    "\n",
    "### Sigmoid Function\n",
    "- Use when: You need a smooth, differentiable function that maps input to the range (0, 1) (for neural networks) or you need a kernel that maps data to infinite dimensions (for SVMs).\n",
    "- Pros: Smooth and differentiable, outputs have nice interpretation as probabilities (for neural networks).\n",
    "- Cons: Can suffer from vanishing gradients problem (for neural networks).\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "- Use when: You need a simple, efficient non-linear function for a neural network.\n",
    "- Pros: Simple and computationally efficient, helps mitigate the vanishing gradients problem.\n",
    "- Cons: Can suffer from \"dead neurons\" where some neurons never activate.\n",
    "\n",
    "### Tanh (Hyperbolic Tangent)\n",
    "- Use when: You need a smooth, differentiable function that maps input to the range (-1, 1) for a neural network.\n",
    "- Pros: Output is zero-centered, making it easier for the model to learn.\n",
    "- Cons: Can still suffer from vanishing gradients problem.\n",
    "\n",
    "### Softmax Function\n",
    "- Use when: You need to output a probability distribution for a multi-class classification problem in a neural network.\n",
    "- Pros: Outputs have a nice interpretation as probabilities, suitable for multi-class classification.\n",
    "- Cons: Computationally more expensive than other activation functions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

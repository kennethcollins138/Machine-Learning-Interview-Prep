{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of Linear Regression as a simple line equation like we did in middle school. You have your equation y = mx + b which plots a relationship between to variables (x and y). The x variable can be viewed as your predictor or \"independent variable\". The other variable, y is your dependent variable. What this means is y's growth is depenent on the growth of x! \n",
    "\n",
    "This is the same exact concept used for linear regression! The model uses statistical methods to return the expected model of y as a linear funciton of x:  \n",
    "y = β0 + β1*x + ε  \n",
    "\n",
    "Here β0 and β1 are the parameters your computer is trying to find out to best fit the relationship of the two variables. β0 is the y inctercept and β1 is the slope of the system. In our model, we are predicting values and our model won't be 100% accurate. For this reason, we need to create an error term, ε, which represents the difference between our actual and predicted values. \n",
    "\n",
    "It is very rare for our system have a linear correlation between data points where this simple explanation can work, but it good for us to start connecting mathematics to these simple concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "For our linear regression, we have a system of linear equations that can be represented in a matrix in the form of y = β0 + β1*x + ε as stated previously.\n",
    "y = β0 + β1*x + ε \n",
    "- y is a nx1 vector of our target values that we will be predicting\n",
    "- X is a nxm matrix of input features we will be plugging in to predict our output\n",
    "- β0 is the y intercept of the regression line. It represents the predicted value of y when our input vector is 0\n",
    "- β1 is the mx1 matrix of parameters we want to estimate for our model.\n",
    "- ε is a n x 1 vector of the error terms\n",
    "\n",
    "Our goal is to find the β0 and β1 that minimizes the sum of the squared residuals. Remember residuals are the difference between the observed, actual values and predicted values of the target variable.  We can achieve this by setting the derivative of the sum of our squared resuiduals with respect to the parameter β1 equal to zero.\n",
    "From this, we are given the solution β1 = (X^t * X)^-1 * (X^t * Y). We have to transpose our vectors due to their dimensions.  ^t and -1 are the transpose and inverse respectively. The stars are just regular matrix multiplication and aren't implying any specfics in particular.\n",
    "\n",
    "### Solving for intercept\n",
    "In the matrix notation of linear regression, y = Xβ + ε, the intercept is included in the β vector. To include the intercept, a column of ones is added to the matrix X. This means that the first element of β (β0) is multiplied by one, so it acts as a constant term in the equation.\n",
    "\n",
    "The parameters β (including β0) are usually estimated by minimizing the sum of squared residuals. This can be done using the normal equation:\n",
    "\n",
    "β = (X'X)^-1 X'y\n",
    "\n",
    "where X' is the transpose of X and (X'X)^-1 is the inverse of X'X. The first element of the resulting β vector will be β0.\n",
    "\n",
    "In the context of your code, once you have solved for β using the QR decomposition and back substitution, the first element of the resulting β vector will be your intercept, β0.\n",
    "\n",
    "### Solving for Slope\n",
    "At this point, we have to calculate the inverse of the matrix which can be computationally expensive and numerically unstable. This is important since the operation of matrix inversion is sensitive to the values in the matrix. If X^t*X is close to being singular(not invertible), small changes can lead to large changes in the inverse which can lead to smlarge changes in the estimated parameters. For this reason, matrix decomposition is super important! We can use it to factor the methods into the product of two or more matrices which makes it way easier to compute the inverse. Methods such QR decomposition can be used to factor our matrix X into the product of an Orthogonal matrix Q and Upper trinagle Matrix R. Once we have this we can solve for β1*x.  \n",
    "\n",
    "X'Xβ = X'Y  \n",
    "QR'QRβ = QR'Y  \n",
    "R'QRβ = R'Q'Y  \n",
    "Rβ = Q'Y  \n",
    "\n",
    "Since R is an upper triangle matrix, we can easily solve this system of equations efficiently using back substitution.\n",
    "Remember there are many other methods such as Cholesky Decomp which can be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Calculus and Optimization\n",
    "[Read/Watch for this portion: Gradient Descent](https://builtin.com/data-science/gradient-descent)  \n",
    "As stated previously, our goal is to find the values of β0 (intercept) and β1 (slope) that minimize the sum of the squared residuals (ε). This is equivalent to minimizing the cost function J(β), defined as:\n",
    "\n",
    "J(β) = 1/2n * Σ(y_i - (β0 + β1*x_i))^2\n",
    "\n",
    "where:\n",
    "- n is the number of observations,\n",
    "- y_i is the observed value of the target variable for observation i,\n",
    "- x_i is the value of the predictor variable for observation i.\n",
    "\n",
    "To find the minimum of J(β), we use vector calculus. Specifically, we calculate the gradient of J(β), which is a vector that contains the partial derivatives of J(β) with respect to each parameter. For simple linear regression, the gradient is a 2-dimensional vector:\n",
    "\n",
    "∇J(β) = [ ∂J/∂β0, ∂J/∂β1 ]\n",
    "\n",
    "The partial derivatives are calculated as follows:\n",
    "\n",
    "∂J/∂β0 = 1/n * Σ(y_i - (β0 + β1*x_i))\n",
    "\n",
    "∂J/∂β1 = 1/n * Σ((y_i - (β0 + β1*x_i)) * x_i)\n",
    "\n",
    "Setting these equal to zero gives us the normal equations, which can be solved to find the parameters that minimize the cost function:\n",
    "\n",
    "Σ(y_i - (β0 + β1*x_i)) = 0\n",
    "\n",
    "Σ((y_i - (β0 + β1*x_i)) * x_i) = 0\n",
    "\n",
    "Solving these equations gives us the values of β0 and β1 that minimize the cost function J(β). This process is known as gradient descent.\n",
    "\n",
    "In each iteration of gradient descent, we update the parameters by subtracting a fraction of the gradient from the current parameter values. The size of this step is determined by the learning rate, a hyperparameter that controls how fast the algorithm converges to the optimal parameters. Utilizing a learning rate is good! However, sometimes we might run into oscillations jumping back and forth across the valley of the cost function without moving towards the minimum. Momentum allows us to mitigate this issue by adding a fraction of the past iteration to the current update vector which kind of acts like a damper for a spring. This would help reduce the oscillations and allow us to converge on the minimum point faster! By doing so, we could potentially reach the local minimum sooner.\n",
    "\n",
    "This iterative process continues until the algorithm converges to a set of parameters that minimize the cost function, i.e., the error between the predicted and actual values is minimized. At this point, the gradient of the cost function is zero (or close to zero), indicating that any small change in the parameters will not significantly decrease the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Geometry\n",
    "Remember our goal in a 2D space (one independent and one target variable) is to find the straight line that best fits these points. This line called the regression line is 1D in shape essentially only projecting a straight line through our plane. The best line mimizes the sum of our squared residuals(mimimizes our cost)! However, it is very rare we will have only one independent variable. For this, we can think of a multi-dimensional spaces where each dimension represents an independent variable. The regression line is now a hyperplane that spans across multiple dimensions!\n",
    "\n",
    "This is where analytical geometry comes in. How are we going to fit our hyperplane in the multi-dimensional space. We can find the shortest distance from each data point to the hyper plane. That distance is the residual! This sum becomes part of the Optimization problem talked about above. There are many methods we use to calculate this but two in particular which are used heavily are L1 and L2 Norm.\n",
    "\n",
    "### **L1 Norm (Manhattan Distance)**: \n",
    "The L1 norm calculates the sum of the absolute differences between two points (or vectors). In a 2D space, it's equivalent to the distance you would travel if you could only move along a grid of horizontal and vertical lines (like in a city block), hence the name Manhattan distance. The formula for L1 norm for two points A(x1, y1) and B(x2, y2) is:\n",
    "\n",
    "\n",
    "L1 = abs(x1 - x2) + abs(y1 - y2)  \n",
    "\n",
    "Pros:  \n",
    "    1. **Sparsity**: L1 norm tends to produce sparse solutions, meaning it tends to produce solutions where many of the coefficients are zero. This can be useful for feature selection, as it can help identify the most important features.\n",
    "    2. **Robustness to Outliers**: L1 norm is more robust to outliers than L2 norm. This is because it does not square the residuals, so large residuals (outliers) do not have as much influence on the solution.\n",
    "\n",
    "Cons:  \n",
    "    1. **Multiple Solutions**: L1 norm can have multiple solutions, especially when the number of features is greater than the number of observations. This can make the solution less stable and harder to interpret.\n",
    "    2. **Computationally Intensive**: Finding the solution can be more computationally intensive than for L2 norm, especially for large datasets.\n",
    "\n",
    "\n",
    "\n",
    "### **L2 Norm (Euclidean Distance)**: \n",
    "The L2 norm calculates the square root of the sum of the squares of the differences between two points (or vectors). In a 2D space, it's equivalent to the straight-line distance between two points, hence the name Euclidean distance. The formula for L2 norm for two points A(x1, y1) and B(x2, y2) is:\n",
    "\n",
    "\n",
    "L2 = sqrt((x1 - x2)^2 + (y1 - y2)^2)  \n",
    "\n",
    "Pros:  \n",
    "    1. **Unique Solution**: L2 norm always has a unique solution, which can make it more stable and easier to interpret than L1 norm.\n",
    "    2. **Efficient Computation**: The solution can be found analytically (i.e., using a formula), which can be more computationally efficient than for L1 norm.\n",
    "\n",
    "Cons:  \n",
    "    1. **Not Robust to Outliers**: L2 norm squares the residuals, so large residuals (outliers) have a large influence on the solution. This can make the solution less robust to outliers than L1 norm.\n",
    "    2. **No Feature Selection**: L2 norm does not produce sparse solutions, so it does not perform feature selection like L1 norm. This can make the solution harder to interpret when there are many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split as tts \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Datasets/50_Startups.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) # n * m array\n",
    "print(y.shape) # n * 1 array\n",
    "# we now know state is a categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough') # allows us to target the column we want to encode\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Variables:\n",
    "- Dummy variables are binary variables representing categories. For a category like \"color\" with values \"red\", \"blue\", \"green\", you'd create dummy variables \"color_red\", \"color_blue\", \"color_green\".\n",
    "- The Dummy Variable Trap is a scenario where dummy variables cause perfect multicollinearity, meaning they are highly correlated. This happens if all dummy variables for a category are included in a regression model.\n",
    "- To avoid the Dummy Variable Trap, one dummy variable for each category should be left out of the model. This left-out category becomes the \"reference category\".\n",
    "- The coefficients on the included dummy variables represent changes relative to the reference category.\n",
    "- Some libraries for regression automatically avoid the Dummy Variable Trap by excluding one category for each set of dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n",
      " [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n",
      " [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n",
      " [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n",
      " [0.0 1.0 0.0 142107.34 91391.77 366168.42]\n",
      " [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n",
      " [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n",
      " [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n",
      " [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n",
      " [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n",
      " [0.0 1.0 0.0 101913.08 110594.11 229160.95]\n",
      " [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n",
      " [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n",
      " [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n",
      " [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n",
      " [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n",
      " [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n",
      " [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n",
      " [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n",
      " [0.0 0.0 1.0 86419.7 153514.11 0.0]\n",
      " [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n",
      " [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n",
      " [0.0 1.0 0.0 73994.56 122782.75 303319.26]\n",
      " [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n",
      " [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n",
      " [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n",
      " [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n",
      " [0.0 0.0 1.0 72107.6 127864.55 353183.81]\n",
      " [0.0 1.0 0.0 66051.52 182645.56 118148.2]\n",
      " [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n",
      " [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n",
      " [0.0 0.0 1.0 61136.38 152701.92 88218.23]\n",
      " [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n",
      " [0.0 1.0 0.0 55493.95 103057.49 214634.81]\n",
      " [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n",
      " [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n",
      " [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n",
      " [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n",
      " [0.0 0.0 1.0 20229.59 65947.93 185265.1]\n",
      " [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n",
      " [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n",
      " [0.0 1.0 0.0 27892.92 84710.77 164470.71]\n",
      " [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n",
      " [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n",
      " [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n",
      " [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n",
      " [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n",
      " [1.0 0.0 0.0 0.0 135426.92 0.0]\n",
      " [0.0 0.0 1.0 542.05 51743.15 0.0]\n",
      " [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n"
     ]
    }
   ],
   "source": [
    "print(X) #New york, california, florida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = LinearRegression(fit_intercept=True, copy_X=True, n_jobs=None, positive=False) # default hyperparamters for tuning model!\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103015.2  103282.38]\n",
      " [132582.28 144259.4 ]\n",
      " [132447.74 146121.95]\n",
      " [ 71976.1   77798.83]\n",
      " [178537.48 191050.39]\n",
      " [116161.24 105008.31]\n",
      " [ 67851.69  81229.06]\n",
      " [ 98791.73  97483.56]\n",
      " [113969.44 110352.25]\n",
      " [167921.07 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2) #2 decimal places\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) #compare predicted and actual values\n",
    "# reshape allows us to create a n x 1 array for easier comparison, last 1 is axis which means itll concatenate along the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9347068473282341\n"
     ]
    }
   ],
   "source": [
    "r_squared = regressor.score(X_test, y_test)\n",
    "print(f'R-squared: {r_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES FOR READER\n",
    "this is a super simple dataset! I didn't modify my hyperparameters at all. I just wanted to connect the relationship between the math and the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

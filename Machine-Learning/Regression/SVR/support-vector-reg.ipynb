{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of SVR is the creation of a hyperplane which approximately captues as many instances as posssible while limiting the deviation for other instances of our data.\n",
    "Imagine we are trying to predict housing prices based off of different parameters, SVR will try t find at most ε away from the actual price for as many points as possible. This can be visualized as a tube fitting around the readius ε to our data. The instances that are within our ε tube are not errors, while ε outside of our tube are.  \n",
    "SVR wants to minimize the volume of this tube which is inherently minimizing error. The support vectors are the points that are on the boundary or edge of our tube. These are the values our model deemed to be most \"sensitive\" to as they define the position and width of the tube. SVR takes on kernels that map the relationship between features and target variables. \n",
    "The intuition is to find a function that fits as many instances as possible within a tolerance (ε) while minimizing the deviations or distance for the remaining data points. \n",
    "**SVR Advantages**\n",
    "- Effective in high dimensional spaces\n",
    "- Memory Efficient: Uses subset of datapoints(support vectors)\n",
    "- Versatile: can model linear and non-linear relationship depending on the kernel we use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated previously, we are looking for a function f(x) that will limit our deviation and maximize the number of instances within our tube while trying to remain as flat as possible. We can think of this as trying to find a line or hyperplane which is at most an ε distance from the actual target value.  \n",
    "\n",
    "Essentially, we can write this as:  \n",
    "|y - f(x)| ≤ ε  \n",
    "\n",
    "This is emphasizing the point that our model is insenstive to the ε tube. This is a desirable property in terms of our margin(distance between closest data points and decision boundary). As stated the ε is the maximum deviation from the target variable, this means that we are accounting for error. With that being said, our model becomes more resistant to noise and outliers. It is important to note that ε does not change the orientation of our decision boundary only the width of the margin.\n",
    "\n",
    "While introducing error, it is important for errors larger than our margin we introduce slack variables. The slack variables measure the deviation outside of the tube.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra and Analytical Geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reference to linear SVR, our hyperplane can be defined as `f(x) = w^T x + b` where `w` is the weight vector, `x` is the input vector, and `b` is the bias term. The objective function is : Minimize (1/2) * ||w||^2 + C * Σ(ξ + ξ*) as stated previously. \n",
    "\n",
    "The term ||w||^2 is the squared Euclidian norm (L2 Norm) of our weight vector which represents the flatness(angle) of our function. Since it is a euclidean norm of a vector V, it is defined as as `sqrt(v1^2 + v2^2 + ... + vn^2)`, so `||w||^2` is simply `w1^2 + w2^2 + ... + wn^2`.\n",
    "The terms `ξ` and `ξ*` are slack variables that measure the deviation outside the ε tube. The sum `Σ(ξ + ξ*)` is simply the sum of these slack variables over all training instances.\n",
    "\n",
    "The constraints of the problem can be written as:\n",
    "\n",
    "\n",
    "y - w^T x - b ≤ ε + ξ  \n",
    "w^T x + b - y ≤ ε + ξ*  \n",
    "ξ, ξ* ≥ 0  \n",
    "\n",
    "\n",
    "These are linear inequalities, and they ensure that the residuals (the differences between the predictions and the actual values) are at most ε, except for the slack variables `ξ` and `ξ*`.  \n",
    "\n",
    "The dual problem of SVR involves the Lagrange multipliers, which are used to incorporate the constraints into the objective function. The solution to the dual problem gives the optimal weight vector `w` and bias term `b`, as well as the support vectors. Will dive into duals in the Optimization section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Optimization and Vector Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the objective function and restraints of SVR from earlier. IT is a convex optimizatio problem as we are trying to find the unique global minimum which minimizes error. The solution to this problem gives the weight fector w and bias term b which help us define our hyper plane. To find these terms, we can use the method of Lagrange multipliers which incorparates the constraints into the objective function.  \n",
    "\n",
    "### The Dual Problem\n",
    "However, when dealing with Lagrange multipliers we run into the dual problem since we are minimizing the Lagrangian with respect to our parameters of the objective function while maximizing our Lagrangian multipliers like the inequality constraints and slack variables.\n",
    "\n",
    "(Looked this up but haven't researched it myself yet, this can be solved using techniques like Sequential Minimal Optimization or SMO)\n",
    "\n",
    "Why is that a problem? We are trying to minimize our original equation by maximizing the Lagrangian multipliers which are based off of the original function.\n",
    "However, the solution to this problem returns the optimal values for our weight and bias.\n",
    "\n",
    "### Advantages of dual problem\n",
    "- it depends on the dot product of the input vectors which allows us to use the kernel trick to map our inputs to a higher dimensional space and solve non-linear problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine, scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Great Resource](https://www.mathworks.com/help/stats/understanding-support-vector-machine-regression.html)  \n",
    "[SMO Algorithm](https://web.stanford.edu/group/SOL/reports/Ma-SMOvsPDCOforSVM.pdf)  \n",
    "[SMO video](https://www.youtube.com/watch?v=Mfp7HQKLSAo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVR:\n",
    "    def __init__(self, C=1.0, epsilon=0.1, kernel='linear'):\n",
    "        self.C = C # Penalty parameter C of the error term\n",
    "        self.epsilon = epsilon # epsilon tube width\n",
    "        self.kernel = kernel # what type of kernel\n",
    "        self.support_vectors = None  # Store the support vectors\n",
    "        self.sv_labels = None  # used to determine the decision boundary, indicates side of decision boundary, position of epsilon tube, and used in prediction\n",
    "        self.sv_alphas = None  # Lagrange multipliers for the support vectors\n",
    "        self.sv_bias = None\n",
    "\n",
    "    # Think about how the data is structured, we dont know if it n x m or n x n, so we need to transpose the second vector to make sure we get the right shape for matrix multiplication\n",
    "    def _linear_kernel(self, X1, X2):\n",
    "        return np.dot(X1, X2.T)\n",
    "\n",
    "    def _compute_kernel(self, X1, X2):\n",
    "        if self.kernel == 'linear':\n",
    "            return self._linear_kernel(X1, X2)\n",
    "        else:\n",
    "            raise ValueError(\"Kernel {} not implemented\".format(self.kernel))\n",
    "\n",
    "    # Lagrainge multipliers returning the error of our constrained optimization problem\n",
    "    def _compute_error(self, X, y, alpha, kernel, bias):\n",
    "        return y - np.dot(alpha * y, kernel) - bias\n",
    "\n",
    "    # Euclidian scored norm(length) of alpa, attempting to find a function that fits the data well and keeps the alpha small\n",
    "    def _compute_objective_function(self, alpha, error):\n",
    "        return 0.5 * np.dot(alpha, alpha) + np.sum(self.epsilon - error if error < 0 else 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        kernel = self._compute_kernel(X, X)\n",
    "\n",
    "        alpha = np.zeros(n_samples)\n",
    "        bias = 0\n",
    "\n",
    "        # Main training loop using SMO\n",
    "        for _ in range(1000):  # maximum iterations\n",
    "            for i in range(n_samples):\n",
    "                error = self._compute_error(X, y, alpha, kernel[i], bias)\n",
    "                \n",
    "                # If error is within the epsilon tube, we don't need to do anything, ensuring optimization respects the constraints\n",
    "                if (y[i] * error < -self.epsilon and alpha[i] < self.C) or (y[i] * error > self.epsilon and alpha[i] > 0):\n",
    "                    j = np.random.choice(list(range(n_samples))) # SMO algorithm \n",
    "                    while j == i:\n",
    "                        j = np.random.choice(list(range(n_samples)))\n",
    "                    error_j = self._compute_error(X, y, alpha, kernel[j], bias)\n",
    "                    alpha_i_old, alpha_j_old = alpha[i], alpha[j]\n",
    "                    if y[i] != y[j]:\n",
    "                        L = max(0, alpha[j] - alpha[i])\n",
    "                        H = min(self.C, self.C + alpha[j] - alpha[i])\n",
    "                    else:\n",
    "                        L = max(0, alpha[i] + alpha[j] - self.C)\n",
    "                        H = min(self.C, alpha[i] + alpha[j])\n",
    "                    if L == H: # if L and H are the same, we can't optimize the problem any further\n",
    "                        continue\n",
    "                    eta = 2 * kernel[i, j] - kernel[i, i] - kernel[j, j] # eta is the similarity between the two samples\n",
    "                    if eta >= 0: # if eta is greater than 0, we can't optimize the problem any further\n",
    "                        continue\n",
    "                    alpha[j] -= y[j] * (error - error_j) / eta\n",
    "                    alpha[j] = max(L, min(H, alpha[j]))\n",
    "                    alpha[i] += y[i] * y[j] * (alpha_j_old - alpha[j])\n",
    "                    b1 = bias - error - y[i] * (alpha[i] - alpha_i_old) * kernel[i, i] - y[j] * (alpha[j] - alpha_j_old) * kernel[i, j]\n",
    "                    b2 = bias - error_j - y[i] * (alpha[i] - alpha_i_old) * kernel[i, j] - y[j] * (alpha[j] - alpha_j_old) * kernel[j, j]\n",
    "                    if 0 < alpha[i] < self.C:\n",
    "                        bias = b1\n",
    "                    elif 0 < alpha[j] < self.C:\n",
    "                        bias = b2\n",
    "                    else:\n",
    "                        bias = (b1 + b2) / 2\n",
    "\n",
    "        # Store support vectors and their corresponding alphas\n",
    "        sv_indices = np.where((alpha > 0) & (alpha < self.C))[0]\n",
    "        self.support_vectors = X[sv_indices]\n",
    "        self.sv_labels = y[sv_indices]\n",
    "        self.sv_alphas = alpha[sv_indices]\n",
    "        self.sv_bias = bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        kernel = self._compute_kernel(X, self.support_vectors)\n",
    "        predictions = np.dot(kernel, self.sv_alphas * self.sv_labels) + self.sv_bias\n",
    "        return predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "- [Overview](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/#:~:text=A%20decision%20tree%20algorithm%20is,each%20node%20of%20the%20tree.)\n",
    "- [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [algorithm](https://www.javatpoint.com/machine-learning-decision-tree-classification-algorithm)\n",
    "- [deeper understanding](https://www.geeksforgeeks.org/decision-tree/)\n",
    "- [gini impurity](https://www.learndatasci.com/glossary/gini-impurity/)\n",
    "- [asm](https://www.tutorialspoint.com/what-is-attribute-selection-measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "- A decision tree classifer splits the data into distinct groups based on certain conditions, much like how a tree branches out. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "- A simplified example of this is a game of guess who. You ask a series of questions to narrow down the posssible options until we reach our best guess. \n",
    "\n",
    "### Important Terminology\n",
    "\n",
    "- **Root Node**: It represents the entire population or sample and this further gets divided into two or more homogeneous sets. At this point, we haven't made any decisions yet.\n",
    "- **Splitting**: It is a process of dividing a node into two or more sub-nodes. For example, does this person have bronw hair? If yes, we split into a group of people with brown hair and those without.\n",
    "- **Decision Node**: When a sub-node splits into further sub-nodes, then it is called a decision node. These will be your inner nodes.\n",
    "- **Leaf/Terminal Node**: Nodes do not split is called Leaf or Terminal node. These are your final predictions.\n",
    "- **Pruning**: When we remove sub-nodes of a decision node, this process is called pruning. You can think of this as removing unnecessary questions. If we know the person has brown hair, we don't need to ask if they have blonde hair.\n",
    "- **Branch/Sub-Tree**: A sub-section of the entire tree is called a branch or sub-tree. We have a branch of brown hair individuals, but within that branch we have a sub-tree of people with brown hair and blue eyes.\n",
    "- **Parent and Child Node**: A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node.\n",
    "\n",
    "## Strength and Weaknesses\n",
    "\n",
    "- **Strengths**\n",
    "    - Simple to understand and interpret. Trees can be visualised.\n",
    "    - Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. \n",
    "    - The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "    - Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable.\n",
    "    - Able to handle multi-output problems.\n",
    "    - Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model, the explanation for the results is typically difficult to understand.\n",
    "    - Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "    - Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "- **Weaknesses**  \n",
    "    - Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "    - Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "    - The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree.\n",
    "    - There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "- **Step 1**: Begin with the entire dataset. This means our root node contains the entire dataset.\n",
    "- **Step 2**: Find the best feature to split the data. This is done by calculating the Gini impurity or entropy of each feature. The feature with the lowest impurity is chosen to split the data.  \n",
    "    - The gini impurity is a measure of how often a randomly chosen element would be incorrectly classified. It is calculated by summing the probability of each item being chosen times the probability of a mistake in categorising that item.  \n",
    "    - The formula for gini impurity is: \n",
    "        - $Gini = 1 - \\sum_{i=1}^{n} p_i^2$\n",
    "        - where $p_i$ is the probability of an item being classified into a particular category.\n",
    "    - The entropy of a dataset is a measure of the amount of uncertainty in the dataset. The entropy of a dataset is zero when the dataset is completely homogeneous.\n",
    "    - The formula for entropy is:\n",
    "        - $Entropy = -\\sum_{i=1}^{n} p_i \\log_2(p_i)$\n",
    "        - where $p_i$ is the probability of an item being classified into a particular category.\n",
    "- **Step 3**: Split the data based on the best feature. This means we create a decision node and two or more leaf nodes.\n",
    "- **Step 4**: Repeat steps 1-3 for each leaf node until we have a tree that is deep enough or we have reached a stopping condition. This could be a maximum depth, minimum number of samples, or a minimum impurity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemening from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "class DecisionNode:\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    '''\n",
    "    Creating a vectorized function that can be applied to an array that checks if x is greater than or equal to a threshold.\n",
    "    The return of this function is a boolean array that can be used to index the original array.\n",
    "    This function is used to split the dataset based on a feature and threshold.\n",
    "    The threshold is given by the best split found by the get_best_feature function.\n",
    "    This function is used to create the true and false branches of a node.\n",
    "    '''\n",
    "    split_func = np.vectorize(lambda x: x >= threshold)\n",
    "    return split_func(X[:, feature_i])\n",
    "\n",
    "'''\n",
    "Impurity functions measure the dispersion of the target variable of the target variable y.\n",
    "These functions can be called within subsets of the data at each node to determine the next split.\n",
    "Remember, the goal is to split impurity or increase the homogenity(or purity) of the target variable.\n",
    "Remember, purity means that the target variable has the same value for all samples in a node. Hence, a clean clasification can be made.\n",
    "'''\n",
    "\n",
    "def gini_impurity(y):\n",
    "    # My numpy implementation of gini impurity\n",
    "    total = len(y)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / total\n",
    "    gini = 1 - np.sum(p**2)\n",
    "    return gini\n",
    "\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / total\n",
    "    entropy = -np.sum(p * np.log2(p + 1e-9))\n",
    "    return entropy\n",
    "\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "\n",
    "def get_best_feature(X, y, impurity_func):\n",
    "    '''\n",
    "    If there is no best feature, the function returns None. This is used to create a leaf node.\n",
    "    '''\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "\n",
    "    current_impurity = impurity_func(y)\n",
    "\n",
    "    # For each feature in the dataset\n",
    "    for feature_i in range(X.shape[1]):\n",
    "        # Get unique values in a sorted order\n",
    "        thresholds = np.unique(X[:, feature_i])\n",
    "        \n",
    "        # For each unique value in the feature\n",
    "        for threshold in thresholds:\n",
    "            # Split the dataset based on the feature and value\n",
    "            split_func = np.vectorize(lambda x: x >= threshold)\n",
    "            y1 = y[split_func(X[:, feature_i])]\n",
    "            y2 = y[~split_func(X[:, feature_i])]\n",
    "\n",
    "            # Calculate the weighted impurity of the two splits\n",
    "            p = float(len(y1)) / (len(y1) + len(y2))\n",
    "            impurity = p * impurity_func(y1) + (1 - p) * impurity_func(y2)\n",
    "\n",
    "            info_gain = current_impurity - impurity\n",
    "\n",
    "            if info_gain > best_gain:\n",
    "                best_gain = info_gain\n",
    "                best_feature = feature_i\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "def build_tree(X, y, impurity_func, max_depth=np.inf, min_size=2, depth=0):\n",
    "    '''\n",
    "    Main function to build the tree. A mask is a boolean array that can be used to index the original array.\n",
    "    Masks are used to split the dataset based on the best feature and threshold.\n",
    "    '''\n",
    "    \n",
    "    best_feature, threshold = get_best_feature(X, y, impurity_func)\n",
    "    \n",
    "    if best_feature is None or depth == max_depth or len(y) <= min_size:\n",
    "        return DecisionNode(value=np.mean(y))\n",
    "\n",
    "    mask = divide_on_feature(X, best_feature, threshold)\n",
    "    true_branch = build_tree(X[mask], y[mask], impurity_func, max_depth, min_size, depth + 1)\n",
    "    false_branch = build_tree(X[~mask], y[~mask], impurity_func, max_depth, min_size, depth + 1)\n",
    "    \n",
    "    return DecisionNode(best_feature, threshold, true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "\n",
    "def predict(node, x):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    if x[node.feature_i] >= node.threshold:\n",
    "        return predict(node.true_branch, x)\n",
    "    else:\n",
    "        return predict(node.false_branch, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

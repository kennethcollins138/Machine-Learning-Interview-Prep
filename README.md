Author: Kenny Collins  
LinkedIn: https://www.linkedin.com/in/kenny-collins-6b0525236/  

# MACHINE LEARNING INTERVIEW PREP
## Purpose

When Preparing for my interviews, I noticed I was not studying as efficiently as I could be. I wanted to create a ground-up comprehensive undestanding of the materials myself. It has been a great help reading dozens of math papers and books, but where am I applying these concepts over in the code? How do concepts such as eigenvectors and matrix decomposition play a role in data compression, and how can I make my code more efficient by understanding this connection? This is why I started this. I want to really push myself to create this guide for myself over the next month or two as I apply.  

## Plan
My plan is to dive into the mathematic topics necessary for Machine Learning. Generally, I would say most of the topics came from the mathematics for machine learning textbook, but I branched off when researching other topics such as gradient boosting. I plan on connecting the math topics to the different ML models explaining the intuition and understanding of how they 'learn'. With that being said, I'm rushing to get this done before I start school next week, so there will 100% be ways for my models to be improved. Anyways, I hope this helps for anyone trying to get a better understanding to the math behind the model! 

For the most part, I tried to keep the structure in the model relatively straight forward with an intuition -> explanation style for mathematics while they build up sequentially from top to bottom. In terms of the Machine Learning Models, I have two files for each type of model. The first will explain the intution behind the model followed by the connection to the math. I plan on coding each type of model from scratch, sklearn, and tensorflow.  

## Disclaimer
By no means am I an expert, I am an undergrand student finishing up his BS in computer science and minor in Applied Mathematics. There will be mistakes, but this is the reason why I'm working on this! I want to find my weaknesses and learn from them. I will update this with the path of recommended study as I expand, and all resources will be linked in here and in each folder and where they are utilized.  

With that being said, I'm not the most organized individual. For the Mathematics folder, I typically relied on feeding ChatGPT my notes and having it return it in an organized manner. I believe it of utmost importance I emphasize this.  

At the end of the day, I hope someone finds the passion in the mathematics behind this like I did! I hope this resource can be a useful and great resource for anyone looking to learn. If you have any questions, don't hesitate to message me on LinkedIn!  

## Notes for later
- build full explanation with math for ML types in TF
- build from scratch in python
- build in scikit
# Resources
- [Mathematics for Machine Learning Textbook](https://mml-book.github.io)
- [Super Data Science](https://www.superdatascience.com) A lot of great resources from here!  
- Github Copilot for a lot of method examples and creating better explanations of my own personal understanding.
- Utilized notes from past courses
- Youtube! Will put a lot of links spaced throughout the notebooks, heavily relied on these channels to connect topics
    - [3Blue1Brown](https://www.youtube.com/@3blue1brown/courses): Great Mathematic visual and intuition!
    - [JohnKrohn](https://www.youtube.com/@JonKrohnLearns/playlists): Great ML resource
    - [Mathemaniac](https://www.youtube.com/@mathemaniac): Great mathematical intution
    - [LazyProgrammer](https://www.youtube.com/@LazyProgrammerOfficial): Great Resource for connecting the math to the code!
- [Deep Learning](https://www.deeplearningbook.org)
- [NN and DL](http://neuralnetworksanddeeplearning.com/index.html)
### Machine Learning
- Will add these resources soon, gotta put them all together
### General Concepts
- [Backpropogation](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
- [Gradient-Based Learning](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)
- [Rectifiers Deep Dive](https://arxiv.org/pdf/1502.01852.pdf)
- [Cross Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)
- [Softmax](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)
- [Problems with Gradient Descent](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)
- [Contrastive Divergence](http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf)
- [Dimensionality Reduction](https://www.cs.toronto.edu/~hinton/science.pdf)
- [Neural Alg of Artistic Style](https://arxiv.org/abs/1508.06576)
- [SSD: Single SHot Multibox Detector](https://arxiv.org/abs/1512.02325)
- [Very Deep convolutional networks for large-scale image recognition VGG](https://arxiv.org/abs/1409.1556)
- [Res Learning for Image Rec](https://arxiv.org/abs/1512.03385)
- [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)
- [Batch Normalizaiton](https://arxiv.org/abs/1502.03167)

### Deep Learning
- [Deep Sparse Rectifier NN](http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)
- [Cost Functions in NN](http://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)
- [NN and DL](http://neuralnetworksanddeeplearning.com/chap2.html)
- [Introduction to CNN](http://cs.nju.edu.cn/wujx/paper/CNN.pdf)
- [Understanding CNN](https://arxiv.org/pdf/1609.04112.pdf)
- [Pooling Operations](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf)
- [Difficulty of RNNs](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf)
- [LSTM](http://www.bioinf.jku.at/publications/older/2604.pdf)
- [Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Effectiveness of RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [Visualizing/Understanding RNNs](https://arxiv.org/pdf/1506.02078.pdf)
- [Valuable LSTM](https://arxiv.org/pdf/1503.04069.pdf)
- [Deep sparse rectifier NN](http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)
- [The SOM](http://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf)
- [Kohonen's SOM](http://www.ai-junkie.com/ann/som/som1.html)
- [Energy Based Learning](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)
- [Fast Alg for Deep Beleif Nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
- [Greedy Laye-Wise Training](http://www.iro.umontreal.ca/~lisa/pointeurs/BengioNips2006All.pdf)
- [Wake-Sleep Alg](http://www.gatsby.ucl.ac.uk/~dayan/papers/hdfn95.pdf)
- [Deep Boltzmann Machines](http://www.utstat.toronto.edu/~rsalakhu/papers/dbm.pdf)
- [Data Compression](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression)
- [Autoencoder in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
- [Sparse Autoencoder Tutorial](http://mccormickml.com/2014/05/30/deep-learning-tutorial-sparse-autoencoder)
- [Sparse Autoencoder](http://www.ericlwilkinson.com/blog/2014/11/19/deep-learning-sparse-autoencoders)
- [k-sparse autoencoder](https://arxiv.org/pdf/1312.5663.pdf)
- [Denoising Autoencoder](http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf)
- [Contractive Auto Encoders](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf)
- [Stacked Denoising Autoencoders](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf)

